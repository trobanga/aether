package integration

import (
	"net/http"
	"net/http/httptest"
	"os"
	"path/filepath"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
	"github.com/trobanga/aether/internal/lib"
	"github.com/trobanga/aether/internal/models"
	"github.com/trobanga/aether/internal/pipeline"
	"github.com/trobanga/aether/internal/services"
)

// TestPipelineImportError_UnreachableURL verifies error handling for unreachable HTTP URLs
func TestPipelineImportError_UnreachableURL(t *testing.T) {
	// Use unreachable URL (connection refused)
	unreachableURL := "http://192.0.2.1:8080/unreachable.ndjson" // 192.0.2.1 is TEST-NET-1, guaranteed non-routable

	// Setup
	tempDir := t.TempDir()
	jobsDir := filepath.Join(tempDir, "jobs")

	config := models.ProjectConfig{
		JobsDir: jobsDir,
		Pipeline: models.PipelineConfig{
			EnabledSteps: []models.StepName{models.StepImport},
		},
		Retry: models.RetryConfig{
			MaxAttempts:      3,
			InitialBackoffMs: 10, // Short backoff for fast test
			MaxBackoffMs:     100,
		},
	}

	logger := lib.NewLogger(lib.LogLevelInfo)
	httpClient := services.NewHTTPClient(2*time.Second, config.Retry, logger)

	// Create job with unreachable URL
	job, err := pipeline.CreateJob(unreachableURL, config, logger)
	require.NoError(t, err, "Job creation should succeed")
	assert.Equal(t, models.InputTypeHTTP, job.InputType, "Input type should be HTTP")

	// Start job
	startedJob := pipeline.StartJob(job)

	// Execute import - should fail
	importedJob, err := pipeline.ExecuteImportStep(startedJob, logger, httpClient, false)

	// Verify error
	assert.Error(t, err, "Import should fail for unreachable URL")
	assert.NotNil(t, importedJob, "Job should be returned even on failure")

	// Verify import step failed
	importStep, found := models.GetStepByName(*importedJob, models.StepImport)
	require.True(t, found, "Import step should exist")
	assert.Equal(t, models.StepStatusFailed, importStep.Status, "Import step should be failed")
	assert.NotNil(t, importStep.LastError, "Should have error details")

	// Network errors should be classified as transient
	assert.Equal(t, models.ErrorTypeTransient, importStep.LastError.Type,
		"Network errors should be transient")

	// Note: Retries happen at the HTTP client level, not at the pipeline step level
	// The HTTP client will retry internally based on its retry config
	// The pipeline step's RetryCount is 0 because the pipeline didn't retry the step,
	// but the HTTP client DID retry the request (we can see this in logs)

	// Verify the error message indicates retries happened
	assert.Contains(t, err.Error(), "failed after", "Error should indicate retry attempts")

	// Verify no partial file was created
	importDir := services.GetJobOutputDir(jobsDir, job.JobID, models.StepImport)
	files, _ := filepath.Glob(filepath.Join(importDir, "*.ndjson"))
	assert.Empty(t, files, "No files should be created on failed download")
}

// TestPipelineImportError_HTTP404 verifies error handling for 404 Not Found
func TestPipelineImportError_HTTP404(t *testing.T) {
	// Create server that returns 404
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusNotFound)
		_, _ = w.Write([]byte("Not Found"))
	}))
	defer server.Close()

	// Setup
	tempDir := t.TempDir()
	jobsDir := filepath.Join(tempDir, "jobs")

	config := models.ProjectConfig{
		JobsDir: jobsDir,
		Pipeline: models.PipelineConfig{
			EnabledSteps: []models.StepName{models.StepImport},
		},
		Retry: models.RetryConfig{
			MaxAttempts:      5,
			InitialBackoffMs: 100,
			MaxBackoffMs:     1000,
		},
	}

	logger := lib.NewLogger(lib.LogLevelInfo)
	httpClient := services.DefaultHTTPClient()

	// Execute import
	job, _ := pipeline.CreateJob(server.URL+"/missing.ndjson", config, logger)
	startedJob := pipeline.StartJob(job)
	importedJob, err := pipeline.ExecuteImportStep(startedJob, logger, httpClient, false)

	// Verify error
	assert.Error(t, err, "Import should fail with 404")
	assert.Contains(t, err.Error(), "404", "Error should mention HTTP status")

	// Verify import step failed
	importStep, _ := models.GetStepByName(*importedJob, models.StepImport)
	assert.Equal(t, models.StepStatusFailed, importStep.Status, "Import step should be failed")

	// 404 is non-transient, so no retries should be attempted
	assert.Equal(t, 0, importStep.RetryCount, "Should not retry non-transient 404 errors")
	assert.Equal(t, models.ErrorTypeNonTransient, importStep.LastError.Type,
		"404 should be non-transient")
}

// TestPipelineImportError_HTTP500WithRetry verifies retry behavior for 500 errors
func TestPipelineImportError_HTTP500WithRetry(t *testing.T) {
	attempts := 0
	maxAttempts := 3

	// Create server that returns 500 for all attempts
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		attempts++
		w.WriteHeader(http.StatusInternalServerError)
		_, _ = w.Write([]byte("Internal Server Error"))
	}))
	defer server.Close()

	// Setup
	tempDir := t.TempDir()
	jobsDir := filepath.Join(tempDir, "jobs")

	config := models.ProjectConfig{
		JobsDir: jobsDir,
		Pipeline: models.PipelineConfig{
			EnabledSteps: []models.StepName{models.StepImport},
		},
		Retry: models.RetryConfig{
			MaxAttempts:      maxAttempts,
			InitialBackoffMs: 10,
			MaxBackoffMs:     100,
		},
	}

	logger := lib.NewLogger(lib.LogLevelInfo)
	httpClient := services.NewHTTPClient(2*time.Second, config.Retry, logger)

	// Execute import
	job, _ := pipeline.CreateJob(server.URL+"/error.ndjson", config, logger)
	startedJob := pipeline.StartJob(job)
	importedJob, err := pipeline.ExecuteImportStep(startedJob, logger, httpClient, false)

	// Verify error after retries
	assert.Error(t, err, "Import should fail after max retries")
	assert.Contains(t, err.Error(), "500", "Error should mention HTTP status")

	// Verify retries were attempted (HTTP client retries internally)
	assert.GreaterOrEqual(t, attempts, maxAttempts,
		"Should attempt download at least max_attempts times")

	// Verify import step failed
	// Note: After HTTP client exhausts retries, the pipeline layer sees it as a final failure
	// The HTTP client DID classify it as transient and retry, but after all retries fail,
	// the pipeline marks it as failed
	importStep, _ := models.GetStepByName(*importedJob, models.StepImport)
	assert.Equal(t, models.StepStatusFailed, importStep.Status)
}

// TestPipelineImportError_InvalidLocalPath verifies error handling for invalid local paths
func TestPipelineImportError_InvalidLocalPath(t *testing.T) {
	tempDir := t.TempDir()
	jobsDir := filepath.Join(tempDir, "jobs")
	invalidPath := filepath.Join(tempDir, "nonexistent", "directory")

	config := models.ProjectConfig{
		JobsDir: jobsDir,
		Pipeline: models.PipelineConfig{
			EnabledSteps: []models.StepName{models.StepImport},
		},
		Retry: models.RetryConfig{
			MaxAttempts:      3,
			InitialBackoffMs: 100,
			MaxBackoffMs:     1000,
		},
	}

	logger := lib.NewLogger(lib.LogLevelInfo)
	httpClient := services.DefaultHTTPClient()

	// Create job with invalid path
	job, err := pipeline.CreateJob(invalidPath, config, logger)
	require.NoError(t, err, "Job creation should succeed")
	assert.Equal(t, models.InputTypeLocal, job.InputType, "Input type should be local")

	// Start and execute
	startedJob := pipeline.StartJob(job)
	importedJob, err := pipeline.ExecuteImportStep(startedJob, logger, httpClient, false)

	// Verify error
	assert.Error(t, err, "Import should fail for nonexistent directory")
	assert.Contains(t, err.Error(), "does not exist", "Error should mention nonexistent path")

	// Verify step failed with non-transient error
	importStep, _ := models.GetStepByName(*importedJob, models.StepImport)
	assert.Equal(t, models.StepStatusFailed, importStep.Status)
	assert.Equal(t, models.ErrorTypeNonTransient, importStep.LastError.Type,
		"File not found should be non-transient")
	assert.Equal(t, 0, importStep.RetryCount, "Should not retry non-transient errors")
}

// TestPipelineImportError_EmptyDirectory verifies error handling for directories with no FHIR files
func TestPipelineImportError_EmptyDirectory(t *testing.T) {
	tempDir := t.TempDir()
	emptyDir := filepath.Join(tempDir, "empty")
	jobsDir := filepath.Join(tempDir, "jobs")

	// Create empty directory
	require.NoError(t, os.MkdirAll(emptyDir, 0755))

	config := models.ProjectConfig{
		JobsDir: jobsDir,
		Pipeline: models.PipelineConfig{
			EnabledSteps: []models.StepName{models.StepImport},
		},
		Retry: models.RetryConfig{
			MaxAttempts:      3,
			InitialBackoffMs: 100,
			MaxBackoffMs:     1000,
		},
	}

	logger := lib.NewLogger(lib.LogLevelInfo)
	httpClient := services.DefaultHTTPClient()

	// Execute import
	job, _ := pipeline.CreateJob(emptyDir, config, logger)
	startedJob := pipeline.StartJob(job)
	importedJob, err := pipeline.ExecuteImportStep(startedJob, logger, httpClient, false)

	// Verify error
	assert.Error(t, err, "Import should fail for empty directory")
	assert.Contains(t, err.Error(), "no FHIR NDJSON files", "Error should mention no FHIR files")

	// Verify step failed
	importStep, _ := models.GetStepByName(*importedJob, models.StepImport)
	assert.Equal(t, models.StepStatusFailed, importStep.Status)
	assert.Equal(t, models.ErrorTypeNonTransient, importStep.LastError.Type)
}

// TestPipelineImportError_PathIsFile verifies error handling when path is a file, not directory
func TestPipelineImportError_PathIsFile(t *testing.T) {
	tempDir := t.TempDir()
	filePath := filepath.Join(tempDir, "file.txt")
	jobsDir := filepath.Join(tempDir, "jobs")

	// Create a file instead of directory
	require.NoError(t, os.WriteFile(filePath, []byte("test"), 0644))

	config := models.ProjectConfig{
		JobsDir: jobsDir,
		Pipeline: models.PipelineConfig{
			EnabledSteps: []models.StepName{models.StepImport},
		},
		Retry: models.RetryConfig{
			MaxAttempts:      3,
			InitialBackoffMs: 100,
			MaxBackoffMs:     1000,
		},
	}

	logger := lib.NewLogger(lib.LogLevelInfo)
	httpClient := services.DefaultHTTPClient()

	// Execute import
	job, _ := pipeline.CreateJob(filePath, config, logger)
	startedJob := pipeline.StartJob(job)
	importedJob, err := pipeline.ExecuteImportStep(startedJob, logger, httpClient, false)

	// Verify error
	assert.Error(t, err, "Import should fail when path is a file")
	assert.Contains(t, err.Error(), "directory", "Error should mention directory issue")

	// Verify step failed
	importStep, _ := models.GetStepByName(*importedJob, models.StepImport)
	assert.Equal(t, models.StepStatusFailed, importStep.Status)
}

// TestPipelineImportError_NetworkTimeout verifies timeout handling
func TestPipelineImportError_NetworkTimeout(t *testing.T) {
	// Create server that delays response beyond timeout
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		time.Sleep(5 * time.Second) // Delay longer than client timeout
		w.WriteHeader(http.StatusOK)
		_, _ = w.Write([]byte(`{"resourceType":"Patient"}`))
	}))
	defer server.Close()

	// Setup
	tempDir := t.TempDir()
	jobsDir := filepath.Join(tempDir, "jobs")

	config := models.ProjectConfig{
		JobsDir: jobsDir,
		Pipeline: models.PipelineConfig{
			EnabledSteps: []models.StepName{models.StepImport},
		},
		Retry: models.RetryConfig{
			MaxAttempts:      2,
			InitialBackoffMs: 10,
			MaxBackoffMs:     100,
		},
	}

	logger := lib.NewLogger(lib.LogLevelInfo)
	// Create HTTP client with short timeout
	httpClient := services.NewHTTPClient(1*time.Second, config.Retry, logger)

	// Execute import
	job, _ := pipeline.CreateJob(server.URL+"/slow.ndjson", config, logger)
	startedJob := pipeline.StartJob(job)
	importedJob, err := pipeline.ExecuteImportStep(startedJob, logger, httpClient, false)

	// Verify timeout error
	assert.Error(t, err, "Import should fail with timeout")

	// Verify step failed with transient error (timeouts are retryable)
	importStep, _ := models.GetStepByName(*importedJob, models.StepImport)
	assert.Equal(t, models.StepStatusFailed, importStep.Status)
	assert.Equal(t, models.ErrorTypeTransient, importStep.LastError.Type,
		"Timeout errors should be transient")
}

// TestPipelineImportError_StatePersistence verifies failed job state is persisted correctly
func TestPipelineImportError_StatePersistence(t *testing.T) {
	// Create server that always fails
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusBadRequest)
		_, _ = w.Write([]byte("Bad Request"))
	}))
	defer server.Close()

	// Setup
	tempDir := t.TempDir()
	jobsDir := filepath.Join(tempDir, "jobs")

	config := models.ProjectConfig{
		JobsDir: jobsDir,
		Pipeline: models.PipelineConfig{
			EnabledSteps: []models.StepName{models.StepImport},
		},
		Retry: models.RetryConfig{
			MaxAttempts:      3,
			InitialBackoffMs: 10,
			MaxBackoffMs:     100,
		},
	}

	logger := lib.NewLogger(lib.LogLevelInfo)
	httpClient := services.DefaultHTTPClient()

	// Execute import
	job, _ := pipeline.CreateJob(server.URL+"/bad.ndjson", config, logger)
	startedJob := pipeline.StartJob(job)
	importedJob, err := pipeline.ExecuteImportStep(startedJob, logger, httpClient, false)

	assert.Error(t, err)

	// Save failed job state
	require.NoError(t, pipeline.UpdateJob(jobsDir, importedJob))

	// Reload job from disk
	reloadedJob, err := pipeline.LoadJob(jobsDir, job.JobID)
	require.NoError(t, err, "Should reload failed job from disk")

	// Verify error state persisted
	assert.Equal(t, importedJob.Status, reloadedJob.Status, "Status should be persisted")
	assert.NotEmpty(t, reloadedJob.ErrorMessage, "Error message should be persisted")

	reloadedImportStep, _ := models.GetStepByName(*reloadedJob, models.StepImport)
	assert.Equal(t, models.StepStatusFailed, reloadedImportStep.Status, "Step status should be persisted")
	assert.NotNil(t, reloadedImportStep.LastError, "Error details should be persisted")
	assert.Equal(t, models.ErrorTypeNonTransient, reloadedImportStep.LastError.Type,
		"Error type should be persisted")
}

// TestPipelineImportError_PartialDownloadCleanup verifies partial files are cleaned up on error
func TestPipelineImportError_PartialDownloadCleanup(t *testing.T) {
	// Create server that fails mid-response
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		// Write partial data then close connection
		_, _ = w.Write([]byte(`{"resourceType":"Patient","id":"1"}
{"resourceType":"Patient","id":"2"}`))
		// Simulate connection drop by using the underlying hijacker
		// (In real test, the connection would be forcibly closed)
	}))
	defer server.Close()

	// Setup
	tempDir := t.TempDir()
	jobsDir := filepath.Join(tempDir, "jobs")

	config := models.ProjectConfig{
		JobsDir: jobsDir,
		Pipeline: models.PipelineConfig{
			EnabledSteps: []models.StepName{models.StepImport},
		},
		Retry: models.RetryConfig{
			MaxAttempts:      2,
			InitialBackoffMs: 10,
			MaxBackoffMs:     100,
		},
	}

	logger := lib.NewLogger(lib.LogLevelInfo)
	httpClient := services.DefaultHTTPClient()

	// Note: This test would ideally simulate connection drop, but for now
	// we test the cleanup mechanism with a different error scenario
	job, _ := pipeline.CreateJob("http://localhost:99999/unreachable.ndjson", config, logger)
	startedJob := pipeline.StartJob(job)
	_, err := pipeline.ExecuteImportStep(startedJob, logger, httpClient, false)

	assert.Error(t, err)

	// Verify no partial files remain
	importDir := services.GetJobOutputDir(jobsDir, job.JobID, models.StepImport)
	files, _ := filepath.Glob(filepath.Join(importDir, "*.ndjson"))
	assert.Empty(t, files, "No partial files should remain after failed download")
}
